---
sidebar_position: 10
---

# Limitations

Though impressive, local Large Language Models (LLMs) come with their own set of challenges. The smaller scale of these models compared to their bigger counterparts can lead to a trade-off in performance. Yet, strides are being made daily to improve upon these limitations. 

### Context Length: A Double-Edged Sword

Context length is the maximum number of tokens (words or parts of words) a model can consider from the input when generating an output. If a text exceeds a model's context length, it gets cut off, and the model misses out on potentially crucial information. The challenge is balancing this length without overwhelming the computational resources. Research and development are ongoing to optimize this factor without compromising performance.

### Censorship & Safety: Navigating the Ethical Terrain

As we traverse this frontier of technology, we find ourselves in the middle of various regulatory and governance challenges, both old and new. Laws concerning the use of AI systems can vary greatly depending on your location and the specific nature of your project, especially when it comes to businesses and startups. It's prudent to consult local laws and assess the need for safeguards or censorship measures within your chosen model, ensuring that unintended or potentially harmful responses are adequately mitigated. Responsibility and ownership are key â€“ any application or project you construct using this technology is under your purview. Comprehending the potential risks before launching a public-interfacing product or service is a critical step towards creating a safe, respectful, and inclusive user experience.

### The Performance Gap: Local vs. Proprietary LLMs

Despite strides in technology, local LLMs have not yet reached the same level of performance as proprietary ones. Proprietary models often have vast resources and data sets at their disposal, which can improve their understanding and generate more accurate responses. However, new FOSS & FOSAI communities are gaining momentum and making rapid progress in developing models that can offer similar capabilities within the constraints of local hardware.

### The Cost of Training

Training an LLM is computationally expensive and can be a significant barrier for individuals or small teams. This cost includes both the financial aspect of procuring the necessary hardware and the time investment needed to fine-tune the model. Efforts are underway to make this process more accessible, including distributed training methods and cost-effective training strategies. Thankfully, new startups like [runpod.io](https://www.runpod.io/) and [vast.ai](https://vast.ai/) allow you to rent-a-gpu from a service provider's cloud, creating an accessible pathway to training your own models if you have the know-how and a few bucks to spare. 

### Access to High-Quality Datasets

Quality data is at the heart of every successful LLM. Access to such data can be challenging and is often compounded by the necessity to clean and preprocess the datasets for optimal results. Despite these hurdles, the FOSS and FOSAI communities around the globe continue to share and create datasets that can be used to train LLMs effectively.  

---

## More Information Coming Soon!

Insights on how to approach and overcome these limitations will be shared on this wiki as we expand on it over time. 

### **This Page Is Still Under Construction!**

Your insights and contributions can help improve this content. Interested? Head over to our [GitHub](https://github.com/UnderstandGPT/UnderstandGPT) repo, read our `contribution.md` file, and make a pull request! Together, we can navigate the challenges and continue pushing the boundaries of what's possible with local LLMs.
